{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['NullHandler',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__version__',\n",
       " '_matutils',\n",
       " 'corpora',\n",
       " 'interfaces',\n",
       " 'logger',\n",
       " 'logging',\n",
       " 'matutils',\n",
       " 'models',\n",
       " 'parsing',\n",
       " 'scripts',\n",
       " 'similarities',\n",
       " 'summarization',\n",
       " 'topic_coherence',\n",
       " 'utils']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "dir(gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 5\n"
     ]
    }
   ],
   "source": [
    "raw_documents = [\"I'm taking the show on the road.\",\n",
    "                 \"My socks are a force multiplier.\",\n",
    "             \"I am the barber who cuts everyone's hair who doesn't cut their own.\",\n",
    "             \"Legend has it that the mind is a mad monkey.\",\n",
    "            \"I make my own fun.\"]\n",
    "print(\"Number of documents:\",len(raw_documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n",
      "[['i', \"'m\", 'taking', 'the', 'show', 'on', 'the', 'road', '.'], ['my', 'socks', 'are', 'a', 'force', 'multiplier', '.'], ['i', 'am', 'the', 'barber', 'who', 'cuts', 'everyone', \"'s\", 'hair', 'who', 'does', \"n't\", 'cut', 'their', 'own', '.'], ['legend', 'has', 'it', 'that', 'the', 'mind', 'is', 'a', 'mad', 'monkey', '.'], ['i', 'make', 'my', 'own', 'fun', '.']]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#nltk.download()\n",
    "\n",
    "gen_docs = [[ word.lower()   for word in word_tokenize(doc)] for doc in raw_documents]\n",
    "#gen_docs = [[    w.lower()   for w in word_tokenize(text)] for text in raw_documents]\n",
    "print(gen_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Example of using Doc2Vec \n",
    "# https://medium.com/@mishra.thedeepak/doc2vec-simple-implementation-example-df2afbbfbad5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import all the dependencies\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['i', 'love', 'machine', 'learning', '.', 'its', 'awesome', '.'], tags=['0']),\n",
       " TaggedDocument(words=['i', 'love', 'coding', 'in', 'python'], tags=['1']),\n",
       " TaggedDocument(words=['i', 'love', 'building', 'chatbots'], tags=['2']),\n",
       " TaggedDocument(words=['they', 'chat', 'amagingly', 'well'], tags=['3'])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\"I love machine learning. Its awesome.\",\n",
    "        \"I love coding in python\",\n",
    "        \"I love building chatbots\",\n",
    "        \"they chat amagingly well\"]\n",
    "\n",
    "# for i , _d in enumerate(data):\n",
    "#     print(str(i) + \" \" + _d)\n",
    "# print(enumerate(data))\n",
    "tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(data)]\n",
    "tagged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Doc2Vec in module gensim.models.doc2vec:\n",
      "\n",
      "class Doc2Vec(gensim.models.base_any2vec.BaseWordEmbeddingsModel)\n",
      " |  Class for training, using and evaluating neural networks described in http://arxiv.org/pdf/1405.4053v2.pdf\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Doc2Vec\n",
      " |      gensim.models.base_any2vec.BaseWordEmbeddingsModel\n",
      " |      gensim.models.base_any2vec.BaseAny2VecModel\n",
      " |      gensim.utils.SaveLoad\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getitem__(self, tag)\n",
      " |  \n",
      " |  __init__(self, documents=None, dm_mean=None, dm=1, dbow_words=0, dm_concat=0, dm_tag_count=1, docvecs=None, docvecs_mapfile=None, comment=None, trim_rule=None, callbacks=(), **kwargs)\n",
      " |      Initialize the model from an iterable of `documents`. Each document is a\n",
      " |      TaggedDocument object that will be used for training.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      documents : iterable of iterables\n",
      " |          The `documents` iterable can be simply a list of TaggedDocument elements, but for larger corpora,\n",
      " |          consider an iterable that streams the documents directly from disk/network.\n",
      " |          If you don't supply `documents`, the model is left uninitialized -- use if\n",
      " |          you plan to initialize it in some other way.\n",
      " |      \n",
      " |      dm : int {1,0}\n",
      " |          Defines the training algorithm. If `dm=1`, 'distributed memory' (PV-DM) is used.\n",
      " |          Otherwise, `distributed bag of words` (PV-DBOW) is employed.\n",
      " |      \n",
      " |      size : int\n",
      " |          Dimensionality of the feature vectors.\n",
      " |      window : int\n",
      " |          The maximum distance between the current and predicted word within a sentence.\n",
      " |      alpha : float\n",
      " |          The initial learning rate.\n",
      " |      min_alpha : float\n",
      " |          Learning rate will linearly drop to `min_alpha` as training progresses.\n",
      " |      seed : int\n",
      " |          Seed for the random number generator. Initial vectors for each word are seeded with a hash of\n",
      " |          the concatenation of word + `str(seed)`. Note that for a fully deterministically-reproducible run,\n",
      " |          you must also limit the model to a single worker thread (`workers=1`), to eliminate ordering jitter\n",
      " |          from OS thread scheduling. (In Python 3, reproducibility between interpreter launches also requires\n",
      " |          use of the `PYTHONHASHSEED` environment variable to control hash randomization).\n",
      " |      min_count : int\n",
      " |          Ignores all words with total frequency lower than this.\n",
      " |      max_vocab_size : int\n",
      " |          Limits the RAM during vocabulary building; if there are more unique\n",
      " |          words than this, then prune the infrequent ones. Every 10 million word types need about 1GB of RAM.\n",
      " |          Set to `None` for no limit.\n",
      " |      sample : float\n",
      " |          The threshold for configuring which higher-frequency words are randomly downsampled,\n",
      " |          useful range is (0, 1e-5).\n",
      " |      workers : int\n",
      " |          Use these many worker threads to train the model (=faster training with multicore machines).\n",
      " |      iter : int\n",
      " |          Number of iterations (epochs) over the corpus.\n",
      " |      hs : int {1,0}\n",
      " |          If 1, hierarchical softmax will be used for model training.\n",
      " |          If set to 0, and `negative` is non-zero, negative sampling will be used.\n",
      " |      negative : int\n",
      " |          If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\"\n",
      " |          should be drawn (usually between 5-20).\n",
      " |          If set to 0, no negative sampling is used.\n",
      " |      dm_mean : int {1,0}\n",
      " |          If 0 , use the sum of the context word vectors. If 1, use the mean.\n",
      " |          Only applies when `dm` is used in non-concatenative mode.\n",
      " |      dm_concat : int {1,0}\n",
      " |          If 1, use concatenation of context vectors rather than sum/average;\n",
      " |          Note concatenation results in a much-larger model, as the input\n",
      " |          is no longer the size of one (sampled or arithmetically combined) word vector, but the\n",
      " |          size of the tag(s) and all words in the context strung together.\n",
      " |      dm_tag_count : int\n",
      " |          Expected constant number of document tags per document, when using\n",
      " |          dm_concat mode; default is 1.\n",
      " |      dbow_words : int {1,0}\n",
      " |          If set to 1 trains word-vectors (in skip-gram fashion) simultaneous with DBOW\n",
      " |          doc-vector training; If 0, only trains doc-vectors (faster).\n",
      " |      trim_rule : function\n",
      " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      " |          Note: The rule, if given, is only used to prune vocabulary during build_vocab() and is not stored as part\n",
      " |          of the model.\n",
      " |      callbacks : :obj: `list` of :obj: `~gensim.models.callbacks.CallbackAny2Vec`\n",
      " |          List of callbacks that need to be executed/run at specific stages during training.\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Abbreviated name reflecting major configuration paramaters.\n",
      " |  \n",
      " |  build_vocab(self, documents, update=False, progress_per=10000, keep_raw_vocab=False, trim_rule=None, **kwargs)\n",
      " |      Build vocabulary from a sequence of sentences (can be a once-only generator stream).\n",
      " |      Each sentence is a iterable of iterables (can simply be a list of unicode strings too).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      documents : iterable of iterables\n",
      " |          The `documents` iterable can be simply a list of TaggedDocument elements, but for larger corpora,\n",
      " |          consider an iterable that streams the documents directly from disk/network.\n",
      " |          See :class:`~gensim.models.doc2vec.TaggedBrownCorpus` or :class:`~gensim.models.doc2vec.TaggedLineDocument`\n",
      " |          in :mod:`~gensim.models.doc2vec` module for such examples.\n",
      " |      keep_raw_vocab : bool\n",
      " |          If not true, delete the raw vocabulary after the scaling is done and free up RAM.\n",
      " |      trim_rule : function\n",
      " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      " |          Note: The rule, if given, is only used to prune vocabulary during build_vocab() and is not stored as part\n",
      " |          of the model.\n",
      " |      progress_per : int\n",
      " |          Indicates how many words to process before showing/updating the progress.\n",
      " |      update : bool\n",
      " |          If true, the new words in `sentences` will be added to model's vocab.\n",
      " |  \n",
      " |  build_vocab_from_freq(self, word_freq, keep_raw_vocab=False, corpus_count=None, trim_rule=None, update=False)\n",
      " |      Build vocabulary from a dictionary of word frequencies.\n",
      " |      Build model vocabulary from a passed dictionary that contains (word,word count).\n",
      " |      Words must be of type unicode strings.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word_freq : dict\n",
      " |          Word,Word_Count dictionary.\n",
      " |      keep_raw_vocab : bool\n",
      " |          If not true, delete the raw vocabulary after the scaling is done and free up RAM.\n",
      " |      corpus_count : int\n",
      " |          Even if no corpus is provided, this argument can set corpus_count explicitly.\n",
      " |      trim_rule : function\n",
      " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      " |          Note: The rule, if given, is only used to prune vocabulary during build_vocab() and is not stored as part\n",
      " |          of the model.\n",
      " |      update : bool\n",
      " |          If true, the new provided words in `word_freq` dict will be added to model's vocab.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from gensim.models.word2vec import Word2Vec\n",
      " |      >>> model= Word2Vec()\n",
      " |      >>> model.build_vocab_from_freq({\"Word1\": 15, \"Word2\": 20})\n",
      " |  \n",
      " |  clear_sims(self)\n",
      " |  \n",
      " |  delete_temporary_training_data(self, keep_doctags_vectors=True, keep_inference=True)\n",
      " |      Discard parameters that are used in training and score. Use if you're sure you're done training a model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      keep_doctags_vectors : bool\n",
      " |          Set `keep_doctags_vectors` to False if you don't want to save doctags vectors,\n",
      " |          in this case you can't to use docvecs's most_similar, similarity etc. methods.\n",
      " |      keep_inference : bool\n",
      " |          Set `keep_inference` to False if you don't want to store parameters that is used for infer_vector method\n",
      " |  \n",
      " |  estimate_memory(self, vocab_size=None, report=None)\n",
      " |      Estimate required memory for a model using current settings.\n",
      " |  \n",
      " |  estimated_lookup_memory(self)\n",
      " |      Estimated memory for tag lookup; 0 if using pure int tags.\n",
      " |  \n",
      " |  infer_vector(self, doc_words, alpha=0.1, min_alpha=0.0001, steps=5)\n",
      " |      Infer a vector for given post-bulk training document.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      doc_words : :obj: `list` of :obj: `str`\n",
      " |          Document should be a list of (word) tokens.\n",
      " |      alpha : float\n",
      " |          The initial learning rate.\n",
      " |      min_alpha : float\n",
      " |          Learning rate will linearly drop to `min_alpha` as training progresses.\n",
      " |      steps : int\n",
      " |          Number of times to train the new document.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :obj: `numpy.ndarray`\n",
      " |          Returns the inferred vector for the new document.\n",
      " |  \n",
      " |  init_sims(self, replace=False)\n",
      " |      Precompute L2-normalized vectors.\n",
      " |      \n",
      " |      If `replace` is set, forget the original vectors and only keep the normalized\n",
      " |      ones = saves lots of memory!\n",
      " |      \n",
      " |      Note that you **cannot continue training or inference** after doing a replace.\n",
      " |      The model becomes effectively read-only = you can call `most_similar`, `similarity`\n",
      " |      etc., but not `train` or `infer_vector`.\n",
      " |  \n",
      " |  reset_from(self, other_model)\n",
      " |      Reuse shareable structures from other_model.\n",
      " |  \n",
      " |  save_word2vec_format(self, fname, doctag_vec=False, word_vec=True, prefix='*dt_', fvocab=None, binary=False)\n",
      " |      Store the input-hidden weight matrix in the same format used by the original\n",
      " |      C word2vec-tool, for compatibility.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          The file path used to save the vectors in.\n",
      " |      doctag_vec : bool\n",
      " |          Indicates whether to store document vectors.\n",
      " |      word_vec : bool\n",
      " |          Indicates whether to store word vectors.\n",
      " |      prefix : str\n",
      " |          Uniquely identifies doctags from word vocab, and avoids collision\n",
      " |          in case of repeated string in doctag and word vocab.\n",
      " |      fvocab : str\n",
      " |          Optional file path used to save the vocabulary\n",
      " |      binary : bool\n",
      " |          If True, the data wil be saved in binary word2vec format, else it will be saved in plain text.\n",
      " |  \n",
      " |  train(self, documents, total_examples=None, total_words=None, epochs=None, start_alpha=None, end_alpha=None, word_count=0, queue_factor=2, report_delay=1.0, callbacks=())\n",
      " |      Update the model's neural weights from a sequence of sentences (can be a once-only generator stream).\n",
      " |      The `documents` iterable can be simply a list of TaggedDocument elements.\n",
      " |      \n",
      " |      To support linear learning-rate decay from (initial) alpha to min_alpha, and accurate\n",
      " |      progress-percentage logging, either total_examples (count of sentences) or total_words (count of\n",
      " |      raw words in sentences) **MUST** be provided (if the corpus is the same as was provided to\n",
      " |      :meth:`~gensim.models.word2vec.Word2Vec.build_vocab()`, the count of examples in that corpus\n",
      " |      will be available in the model's :attr:`corpus_count` property).\n",
      " |      \n",
      " |      To avoid common mistakes around the model's ability to do multiple training passes itself, an\n",
      " |      explicit `epochs` argument **MUST** be provided. In the common and recommended case,\n",
      " |      where :meth:`~gensim.models.word2vec.Word2Vec.train()` is only called once,\n",
      " |      the model's cached `iter` value should be supplied as `epochs` value.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      documents : iterable of iterables\n",
      " |          The `documents` iterable can be simply a list of TaggedDocument elements, but for larger corpora,\n",
      " |          consider an iterable that streams the documents directly from disk/network.\n",
      " |          See :class:`~gensim.models.doc2vec.TaggedBrownCorpus` or :class:`~gensim.models.doc2vec.TaggedLineDocument`\n",
      " |          in :mod:`~gensim.models.doc2vec` module for such examples.\n",
      " |      total_examples : int\n",
      " |          Count of sentences.\n",
      " |      total_words : int\n",
      " |          Count of raw words in documents.\n",
      " |      epochs : int\n",
      " |          Number of iterations (epochs) over the corpus.\n",
      " |      start_alpha : float\n",
      " |          Initial learning rate.\n",
      " |      end_alpha : float\n",
      " |          Final learning rate. Drops linearly from `start_alpha`.\n",
      " |      word_count : int\n",
      " |          Count of words already trained. Set this to 0 for the usual\n",
      " |          case of training on all words in sentences.\n",
      " |      queue_factor : int\n",
      " |          Multiplier for size of queue (number of workers * queue_factor).\n",
      " |      report_delay : float\n",
      " |          Seconds to wait before reporting progress.\n",
      " |      callbacks : :obj: `list` of :obj: `~gensim.models.callbacks.CallbackAny2Vec`\n",
      " |          List of callbacks that need to be executed/run at specific stages during training.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  load(*args, **kwargs) from builtins.type\n",
      " |      Load a previously saved object (using :meth:`~gensim.utils.SaveLoad.save`) from file.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to file that contains needed object.\n",
      " |      mmap : str, optional\n",
      " |          Memory-map option.  If the object was saved with large arrays stored separately, you can load these arrays\n",
      " |          via mmap (shared memory) using `mmap='r'.\n",
      " |          If the file being loaded is compressed (either '.gz' or '.bz2'), then `mmap=None` **must be** set.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.utils.SaveLoad.save`\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      object\n",
      " |          Object loaded from `fname`.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      IOError\n",
      " |          When methods are called on instance (should be called from class).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  dbow\n",
      " |      int {1,0} : `dbow=1` indicates `distributed bag of words` (PV-DBOW) else\n",
      " |      'distributed memory' (PV-DM) is used.\n",
      " |  \n",
      " |  dm\n",
      " |      int {1,0} : `dm=1` indicates 'distributed memory' (PV-DM) else\n",
      " |      `distributed bag of words` (PV-DBOW) is used.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.models.base_any2vec.BaseWordEmbeddingsModel:\n",
      " |  \n",
      " |  doesnt_match(self, words)\n",
      " |      Deprecated. Use self.wv.doesnt_match() instead.\n",
      " |      Refer to the documentation for `gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.doesnt_match`\n",
      " |  \n",
      " |  evaluate_word_pairs(self, pairs, delimiter='\\t', restrict_vocab=300000, case_insensitive=True, dummy4unknown=False)\n",
      " |      Deprecated. Use self.wv.evaluate_word_pairs() instead.\n",
      " |      Refer to the documentation for `gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.evaluate_word_pairs`\n",
      " |  \n",
      " |  most_similar(self, positive=None, negative=None, topn=10, restrict_vocab=None, indexer=None)\n",
      " |      Deprecated. Use self.wv.most_similar() instead.\n",
      " |      Refer to the documentation for `gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar`\n",
      " |  \n",
      " |  most_similar_cosmul(self, positive=None, negative=None, topn=10)\n",
      " |      Deprecated. Use self.wv.most_similar_cosmul() instead.\n",
      " |      Refer to the documentation for `gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar_cosmul`\n",
      " |  \n",
      " |  n_similarity(self, ws1, ws2)\n",
      " |      Deprecated. Use self.wv.n_similarity() instead.\n",
      " |      Refer to the documentation for `gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.n_similarity`\n",
      " |  \n",
      " |  similar_by_vector(self, vector, topn=10, restrict_vocab=None)\n",
      " |      Deprecated. Use self.wv.similar_by_vector() instead.\n",
      " |      Refer to the documentation for `gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similar_by_vector`\n",
      " |  \n",
      " |  similar_by_word(self, word, topn=10, restrict_vocab=None)\n",
      " |      Deprecated. Use self.wv.similar_by_word() instead.\n",
      " |      Refer to the documentation for `gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similar_by_word`\n",
      " |  \n",
      " |  similarity(self, w1, w2)\n",
      " |      Deprecated. Use self.wv.similarity() instead.\n",
      " |      Refer to the documentation for `gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similarity`\n",
      " |  \n",
      " |  wmdistance(self, document1, document2)\n",
      " |      Deprecated. Use self.wv.wmdistance() instead.\n",
      " |      Refer to the documentation for `gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.wmdistance`\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.models.base_any2vec.BaseWordEmbeddingsModel:\n",
      " |  \n",
      " |  cum_table\n",
      " |  \n",
      " |  hashfxn\n",
      " |  \n",
      " |  iter\n",
      " |  \n",
      " |  layer1_size\n",
      " |  \n",
      " |  min_count\n",
      " |  \n",
      " |  sample\n",
      " |  \n",
      " |  syn0_lockf\n",
      " |  \n",
      " |  syn1\n",
      " |  \n",
      " |  syn1neg\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.models.base_any2vec.BaseAny2VecModel:\n",
      " |  \n",
      " |  save(self, fname_or_handle, **kwargs)\n",
      " |      Save the object to file.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname_or_handle : str or file-like\n",
      " |          Path to output file or already opened file-like object. If the object is a file handle,\n",
      " |          no special array handling will be performed, all attributes will be saved to the same file.\n",
      " |      separately : list of str or None, optional\n",
      " |          If None -  automatically detect large numpy/scipy.sparse arrays in the object being stored, and store\n",
      " |          them into separate files. This avoids pickle memory errors and allows mmap'ing large arrays\n",
      " |          back on load efficiently.\n",
      " |          If list of str - this attributes will be stored in separate files, the automatic check\n",
      " |          is not performed in this case.\n",
      " |      sep_limit : int\n",
      " |          Limit for automatic separation.\n",
      " |      ignore : frozenset of str\n",
      " |          Attributes that shouldn't be serialize/store.\n",
      " |      pickle_protocol : int\n",
      " |          Protocol number for pickle.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.utils.SaveLoad.load`\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Doc2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 100\n",
    "vec_size = 20\n",
    "alpha = 0.025\n",
    "\n",
    "model = Doc2Vec(vector_size=vec_size,\n",
    "                alpha=alpha, \n",
    "                min_alpha=0.00025,\n",
    "                min_count=1,\n",
    "                dm =1)\n",
    "  \n",
    "model.build_vocab(tagged_data)\n",
    "\n",
    "# print(model.iter)\n",
    "# print(model.corpus_count)\n",
    "for epoch in range(max_epochs):\n",
    "    print('iteration {0}'.format(epoch))\n",
    "    model.train(tagged_data,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.iter)\n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "\n",
    "model.save(\"d2v.model\")\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V1_infer [-0.04268433  0.0284607  -0.02886547 -0.03692101  0.04286352 -0.03328845\n",
      " -0.04455069 -0.00265194  0.00525941 -0.00592807 -0.00865921 -0.04874882\n",
      "  0.00348158  0.00394442  0.06250465 -0.01305453  0.01819889 -0.0057979\n",
      "  0.01867749  0.06264462]\n",
      "20\n",
      "[('0', 0.9948084354400635), ('2', 0.9887635707855225), ('3', 0.9861475229263306)]\n",
      "[-0.16012873  0.22125486 -0.21141519 -0.46688324  0.32606333 -0.10335435\n",
      " -0.31755304  0.04725485  0.03098422 -0.17313875  0.08604704 -0.23521224\n",
      " -0.07656001 -0.20472744  0.34628564  0.00432583  0.23865759 -0.04606352\n",
      "  0.31415084  0.5443294 ]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "\n",
    "model= Doc2Vec.load(\"d2v.model\")\n",
    "#to find the vector of a document which is not in training data\n",
    "test_data = word_tokenize(\"I love chatbots\".lower())\n",
    "v1 = model.infer_vector(test_data)\n",
    "print(\"V1_infer\", v1)\n",
    "print(len(v1))\n",
    "\n",
    "# to find most similar doc using tags\n",
    "similar_doc = model.docvecs.most_similar('1')\n",
    "print(similar_doc)\n",
    "\n",
    "\n",
    "# to find vector of doc in training data using tags or in other words, printing the vector of document at index 1 in training data\n",
    "print(model.docvecs['1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Understanding Python \n",
    "import pandas as pd\n",
    "\n",
    "def do_something(df):\n",
    "    foo = df[['bar', 'baz']]  # Is foo a view? A copy? Nobody knows!\n",
    "    print(foo)\n",
    "    # ... many lines here ...\n",
    "    foo['quux'] = value       # We don't know whether this will modify df or not!\n",
    "    return foo\n",
    "\n",
    "print(do_something(pd.Dataframe()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
