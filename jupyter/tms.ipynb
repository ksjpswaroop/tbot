{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guidelines to reading this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This notebook is divided into following key sections \n",
    "    - PREPROCESS : Pre-processing & clean data on data to extract relevant features. \n",
    "    \n",
    "    - MODEL : Train a model what can give recommendations. We have created 2 models. One to train over cat_item & other\n",
    "     to train over RBC Line Item title (LIT). cat_item is much cleaner than RBC LIT & forms a large chunk of tickets we          recommend (approximately 75 %) . For tickets that don't have cat_item, we use the RBC Line Item Title as the key feature. \n",
    "     \n",
    "    - POST PROCESS : Filter the results to ensure analyst are assigned tickets based on the teams they work for. The analyst      have less then or equal to 45 tickets in their queue. The analyst is not on a vacation. \n",
    "    \n",
    "- For every function I specify what the function does. This is followed by input parameters (params) takes by the functions & \n",
    "  output it returns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Below are the python modules we would be using. \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, glob, sys , re\n",
    "import sklearn , pickle\n",
    "from datetime import datetime\n",
    "from dateutil import relativedelta\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Below are the various locations where we look for various files, which have the data required by the bot. You need make sure you set these paths appropriately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ['PROJECT_DIR'] = \"C:\\\\Users\\\\581686284\\\\PycharmProjects\\\\Automated_Ticket_Management-2\\\\\"\n",
    "os.environ['SM9_TRAINING_DATA_EXCEL'] = \"rawdata\\\\sm9\\\\train_excel\\\\*.xlsx\"\n",
    "os.environ['SM9_TEST_DATA_EXCEL'] = \"rawdata\\\\sm9\\\\test_excel\\\\*.xlsx\"\n",
    "os.environ['SERVICE_NOW_TRAINING_DATA_EXCEL'] = \"rawdata\\\\serviceNow\\\\train_excel\\\\*.xlsx\"\n",
    "os.environ['SERVICE_NOW_TEST_DATA_EXCEL'] = \"rawdata\\\\serviceNow\\\\test_excel\\\\*.xlsx\"\n",
    "os.environ['RECOMMENDATIONS'] = \"recommendations\\\\\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Utility Methods that are required. Below is a short description of what it does & why is it needed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# object_to_int : Used when model is pre-processed. Converts float (.0) & NaN's to integer for Analyst Employee Id. Analyst Id's \n",
    "# are numbers , not floating point. If number is not an integer, we're presently setting to an arbitrary value. Such values would \n",
    "# be removed later as we only consider analyst that are part of the team & have been assigned tickets. \n",
    "\n",
    "def object_to_int(x):\n",
    "    try:\n",
    "        return int(x)\n",
    "    except ValueError:\n",
    "        return 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# getAnalyst : Gives all the analyst we have in ESAM Operations as per the 'Team List.xlsx' file. We remove FTE Team Lead, as they \n",
    "# don't work on tickets. This method returns employee id's as an index , along with analyst's name, skills  & status. We'll\n",
    "# refer to this as ESAM Operations Roaster, to avoid mixing it with the various internal teams (e.g : Windows, Insurance etc)\n",
    "# we have. \n",
    "\n",
    "def getAnalyst():\n",
    "    team_data_columns = ['Name','Status','Employee #','Windows & Appliciation (Line of Business) Skillset']\n",
    "    team_sheet = pd.read_excel(os.environ['PROJECT_DIR'] + 'rawdata\\\\Team List.xlsx' , header=1 , usecols = team_data_columns, index_col = 2)\n",
    "    team_sheet_without_leads = team_sheet.loc[team_sheet['Status'].str.strip() != 'FTE Team Lead']\n",
    "    analyst_emp_id = team_sheet_without_leads.index.tolist()\n",
    "    return team_sheet_without_leads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting team_sheet_without_leads as a global variable as this information is required by multiple functions. \n",
    "team_sheet_without_leads = getAnalyst()\n",
    "# team_dict = getTeams()\n",
    "# team_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SM9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Need to pass it a parameter to recognize whether we're dealing with training or test data. \n",
    "# This function reads the SM9 data we have. Focuses on columns listed in sm9_columns. Its MANDATORY to ensure the columns are \n",
    "# listed exactly this way in the excels we read. We're using excel over csv, as it takes less space & we have a size constraint,\n",
    "# when deploying on PCF Cloud. We pre-process the data to focus only on tickets in Windows & Applications bin. Only those tickets\n",
    "# that have been assigned to analyst, so that the bot can learn from it. We ensure the analyst employee id's are integers, not \n",
    "# floats. We filter the tickets to focus only on tickets that have been assigned to analyst part of the current roaster & not \n",
    "# others who are part of the team. Hence its critical we have all analyst working with the team in the ESAM Roaster.  \n",
    "# We assign tickets based on teams an analyst is part of, we use Client Base to map to the appropriate team. If its tickets raised\n",
    "# for Windows bin & Client Base is empty, then we set the Client Base as Windows. This makes it possible to assigns tickets \n",
    "# raised for Windows bin to analyst working in Windows Team. For some tickets the RBC Line Item Title is missing. In such as case\n",
    "# we set the RBC Line Item Title to the RBC Description. We use RBC Line Item Title to make recommendations for tickets that \n",
    "# dont have a cat_item value (cat_item is present in Service Now). \n",
    "\n",
    "# Input Params : This function takes as input the location of the directory where it should look for fetch SM9 data (both Windows & Applications)\n",
    "# Output : The SM9 tickets which have RITM & those which don't have RITM. \n",
    "\n",
    "def getSM9TrainData(location):\n",
    "    #sm9_files = glob.glob(os.environ['PROJECT_DIR'] +  location)\n",
    "    print('Started getSM9TrainData')\n",
    "    sm9_files = glob.glob(location)\n",
    "    sm9_columns = ['Assigned Dept', 'Assigned to', 'Number','RBC Line Item Title', 'RBCMMPRITM', 'Rbc Description', 'Client Base']\n",
    "    # Read tickets in Excel file\n",
    "    sm9_data_list = [ pd.read_excel(sm9_file) for sm9_file in sm9_files ]\n",
    "    sm9_data = pd.concat(sm9_data_list)\n",
    "    sm9_data = sm9_data[sm9_columns]\n",
    "    sm9_data = sm9_data.reset_index(drop=True)\n",
    "    # Consider only Windows & Application tickets\n",
    "    sm9_data = sm9_data.loc[sm9_data['Assigned Dept'].isin(['EAA_WINDOWS SERVICES_IMPL' , 'EAA_APPLICATION SERVICES_IMPL'])]\n",
    "    # Remove those tickets that don't have Analyst linked to it. Noise. \n",
    "    sm9_data = sm9_data[pd.notnull(sm9_data['Assigned to'])]\n",
    "    # Convert objects (float values in particular) to int\n",
    "    sm9_data['Assigned to'] = sm9_data['Assigned to'].apply(object_to_int)\n",
    "    # Give me only those tickets, who are assigned to analyst who are part of the current team. \n",
    "    analyst_emp_id = team_sheet_without_leads.index.tolist()\n",
    "    sm9_data = sm9_data.loc[sm9_data['Assigned to'].isin(analyst_emp_id)]\n",
    "    # If RBC Line Item Title is null , then set replace it with RBC Description. \n",
    "    tickets_with_no_title = sm9_data.loc[sm9_data['RBC Line Item Title'].isnull()].index.tolist()\n",
    "    sm9_data.loc[tickets_with_no_title,'RBC Line Item Title'] = sm9_data.loc[tickets_with_no_title,'Rbc Description']\n",
    "    sm9_data_without_RITM = sm9_data.loc[sm9_data['RBCMMPRITM'].isnull()]\n",
    "    sm9_data_with_RITM = sm9_data.loc[~sm9_data['RBCMMPRITM'].isnull()]\n",
    "    print('Done getSM9TrainData')\n",
    "    return sm9_data_with_RITM , sm9_data_without_RITM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getSM9TestData(location):\n",
    "    print('Started getSM9TestData')\n",
    "    sm9_files = glob.glob(location)\n",
    "    sm9_columns = ['Assigned Dept', 'Number','RBC Line Item Title', 'RBCMMPRITM', 'Rbc Description', 'Client Base']\n",
    "    #sm9_data_list = [ pd.read_csv(sm9_file , encoding='latin-1' , usecols=sm9_columns ) for sm9_file in sm9_files ]\n",
    "    # Read tickets in CSV file\n",
    "    #sm9_data_list = [ pd.read_csv(sm9_file , encoding='latin-1' ) for sm9_file in sm9_files ]\n",
    "    # Read tickets in Excel file\n",
    "    sm9_data_list = [ pd.read_excel(sm9_file) for sm9_file in sm9_files ]\n",
    "    sm9_data = pd.concat(sm9_data_list)\n",
    "    sm9_data = sm9_data[sm9_columns]\n",
    "    sm9_data = sm9_data.reset_index(drop=True)\n",
    "    # Consider only Windows & Application tickets\n",
    "    sm9_data = sm9_data.loc[sm9_data['Assigned Dept'].isin(['EAA_WINDOWS SERVICES_IMPL' , 'EAA_APPLICATION SERVICES_IMPL'])]\n",
    "    # Set client base as Windows for tickets that have null Client Base & Assigned Dept as EAA_WINDOWS SERVICES_IMPL\n",
    "    win_tickets_null_client_base = sm9_data.loc[(sm9_data['Client Base'].isnull()) & (sm9_data['Assigned Dept'] == 'EAA_WINDOWS SERVICES_IMPL')].index.tolist()\n",
    "    sm9_data.loc[win_tickets_null_client_base , 'Client Base'] = 'Windows'\n",
    "    # If RBC Line Item Title is null , then set replace it with RBC Description. \n",
    "    tickets_with_no_title = sm9_data.loc[sm9_data['RBC Line Item Title'].isnull()].index.tolist()\n",
    "    sm9_data.loc[tickets_with_no_title,'RBC Line Item Title'] = sm9_data.loc[tickets_with_no_title,'Rbc Description']\n",
    "    sm9_data_without_RITM = sm9_data.loc[sm9_data['RBCMMPRITM'].isnull()]\n",
    "    sm9_data_with_RITM = sm9_data.loc[~sm9_data['RBCMMPRITM'].isnull()]\n",
    "    print('Ended getSM9TestData')\n",
    "    return sm9_data_with_RITM , sm9_data_without_RITM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This method clean \n",
    "def cleanTicketTitle(data):\n",
    "    print('Started cleanTicketTitle')\n",
    "    data['RBC Line Item Title'] = data['RBC Line Item Title'].str.replace(r\"\\d+/\\d+\",'')\n",
    "    data['RBC Line Item Title'] = data['RBC Line Item Title'].str.replace(r\"[0-9]\",'')\n",
    "    #sm9_data_without_RITM['RBC Line Item Title'] = sm9_data_without_RITM['RBC Line Item Title'].str.replace(r\"[0-9]\",'')\n",
    "    data['RBC Line Item Title'] = data['RBC Line Item Title'].str.replace(r\"A termination request has recently been submitted on behalf of RBC Access Manager for .*\",'termination request Access Manager')\n",
    "    data['RBC Line Item Title'] = data['RBC Line Item Title'].str.replace(r\"Access Manager manual termination request submitted .*\",'Access Manager manual termination')\n",
    "    data['RBC Line Item Title'] = data['RBC Line Item Title'].str.replace(r\"An access review has recently been completed .*\",'access review completed')\n",
    "    data['RBC Line Item Title'] = data['RBC Line Item Title'].str.replace(r'[:.,-/?*[\\]&+()!]',' ')\n",
    "    data['RBC Line Item Title'] = data['RBC Line Item Title'].str.replace(r'( ESC | PA | PO | PC | PD| Apr | April )','')\n",
    "    data['RBC Line Item Title'] = data['RBC Line Item Title'].str.replace(r'  ',' ')\n",
    "    data['RBC Line Item Title'] = data['RBC Line Item Title'].str.strip()\n",
    "    print('Ended cleanTicketTitle')\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Service Now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Service Now does not have Client Base. How are we going to proceed once Service Now goes live. \n",
    "# Changes to be made once Service Now is up : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Need to pass it a parameter to recognize whether we're dealing with training or test data. \n",
    "def getServiceNowTrainData(location):\n",
    "    print('Started getServiceNowTrainData')\n",
    "    #sn_files = glob.glob(os.environ['PROJECT_DIR'] +  location)\n",
    "    sn_files = glob.glob(location)\n",
    "    #sn_files\n",
    "    sn_columns = ['cat_item' , 'number' , 'u_assignment_group', 'state' , 'stage' ]\n",
    "#     reqd_stages = ['Fulfillment']\n",
    "#     reqd_states = ['Work in Progress','Open']\n",
    "    #sn_df = [pd.read_csv(sn_file , encoding='latin-1' , usecols=sn_columns) for sn_file in sn_files]\n",
    "    sn_df = [pd.read_excel(sn_file , usecols=sn_columns) for sn_file in sn_files]\n",
    "    sn_data = pd.concat(sn_df)\n",
    "    sn_data = sn_data.loc[sn_data['u_assignment_group'].isin(['EAA_WINDOWS SERVICES_IMPL','EAA_APPLICATION SERVICES_IMPL'])]\n",
    "    #sn_data = sn_data.loc[sn_data['state'].str.strip().isin(reqd_states) | sn_data['stage'].str.strip().isin(reqd_stages)]\n",
    "    print('Ended getServiceNowTrainData')\n",
    "    return sn_data\n",
    "    #sn_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Filter Service Now data by "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getServiceNowTestData(location):\n",
    "    print('Started getServiceNowTestData')\n",
    "    sn_files = glob.glob(location)\n",
    "    #sn_files\n",
    "    sn_columns = ['cat_item' , 'number' , 'u_assignment_group', 'state' , 'stage' ]\n",
    "    reqd_stages = ['Fulfillment']\n",
    "    reqd_states = ['Work in Progress','Open']\n",
    "    #sn_df = [pd.read_csv(sn_file , encoding='latin-1' , usecols=sn_columns) for sn_file in sn_files]\n",
    "    sn_df = [pd.read_excel(sn_file , usecols=sn_columns) for sn_file in sn_files]\n",
    "    sn_data = pd.concat(sn_df)\n",
    "    sn_data = sn_data.loc[sn_data['u_assignment_group'].isin(['EAA_WINDOWS SERVICES_IMPL','EAA_APPLICATION SERVICES_IMPL'])]\n",
    "    sn_data = sn_data.loc[sn_data['state'].str.strip().isin(reqd_states) | sn_data['stage'].str.strip().isin(reqd_stages)]\n",
    "    print(\"sn_data.shape : \", sn_data.shape)\n",
    "    print('Ended getServiceNowTestData')\n",
    "    return sn_data\n",
    "    #sn_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mergeTrainingData(sm9_data_with_RITM,sn_data):\n",
    "#     sm9_data_with_RITM , sm9_data_without_RITM = getSM9TrainData()\n",
    "#     sn_data = getServiceNowTrainData()\n",
    "    print('Started mergeTrainingData')\n",
    "    merged_data = pd.merge(sn_data ,sm9_data_with_RITM, left_on = 'number' , right_on='RBCMMPRITM' , how='outer', indicator=True)\n",
    "    # Only consider tickets assigned to analyst. Remove tickets that are auto-completed. \n",
    "    merged_data = merged_data.loc[~merged_data['Assigned to'].isnull()]\n",
    "    # Convert Assigned to from float to int. \n",
    "    merged_data['Assigned to'] = merged_data['Assigned to'].apply(object_to_int)\n",
    "    # You might need RBC Description for cases when RBC Line Item Title is null. \n",
    "    merged_data = merged_data[['number' , 'Number' , 'cat_item' , 'u_assignment_group' , 'Assigned to' , 'RBC Line Item Title', 'Rbc Description' , 'Client Base' , 'state' , 'stage']]\n",
    "    merged_data_without_cat_item = merged_data.loc[merged_data['cat_item'].isnull()]\n",
    "    merged_data_with_cat_item = merged_data.loc[~merged_data['cat_item'].isnull()]\n",
    "    \n",
    "    \n",
    "    \n",
    "    #merged_data_by_title = mergeDataByRBCTitle(sm9_data_without_RITM,merged_data_without_cat_item)\n",
    "    print('Ended mergeTrainingData')\n",
    "    return merged_data_with_cat_item , merged_data_without_cat_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mergeTestData(sm9_data_with_RITM,sn_data):\n",
    "#     sm9_data_with_RITM , sm9_data_without_RITM = getSM9TrainData()\n",
    "#     sn_data = getServiceNowTrainData()\n",
    "    print('Started mergeTestData')\n",
    "    merged_data = pd.merge(sn_data ,sm9_data_with_RITM, left_on = 'number' , right_on='RBCMMPRITM' , how='outer', indicator=True)\n",
    "    # Only consider tickets assigned to analyst. Remove tickets that are auto-completed. \n",
    "    #merged_data = merged_data.loc[~merged_data['Assigned to'].isnull()]\n",
    "    # Convert Assigned to from float to int. \n",
    "    #merged_data['Assigned to'] = merged_data['Assigned to'].apply(object_to_int)\n",
    "    # You might need RBC Description for cases when RBC Line Item Title is null. \n",
    "    merged_data = merged_data[['Assigned Dept','number' , 'Number' , 'cat_item' , 'u_assignment_group' , 'RBC Line Item Title' , 'Rbc Description' , 'Client Base', 'state' , 'stage' ]]\n",
    "    merged_data_without_cat_item = merged_data.loc[merged_data['cat_item'].isnull()]\n",
    "    merged_data_with_cat_item = merged_data.loc[~merged_data['cat_item'].isnull()]\n",
    "    #merged_data_by_title = mergeDataByRBCTitle(sm9_data_without_RITM,merged_data_without_cat_item)\n",
    "    \n",
    "    # Set client base as Windows for tickets that have null Client Base & Assigned Dept as EAA_WINDOWS SERVICES_IMPL\n",
    "    win_tickets_null_client_base = merged_data_with_cat_item.loc[(merged_data_with_cat_item['Client Base'].str.strip().isnull()) & ((merged_data_with_cat_item['Assigned Dept'].str.strip() == 'EAA_WINDOWS SERVICES_IMPL') | (merged_data_with_cat_item['u_assignment_group'].str.strip() == 'EAA_WINDOWS SERVICES_IMPL'))].index.tolist()\n",
    "    merged_data_with_cat_item.loc[win_tickets_null_client_base , 'Client Base'] = 'Windows'\n",
    "    \n",
    "    # Set client base as Windows for tickets that have null Client Base & Assigned Dept as EAA_WINDOWS SERVICES_IMPL\n",
    "    win_tickets_null_client_base = merged_data_without_cat_item.loc[(merged_data_without_cat_item['Client Base'].str.strip().isnull()) & ((merged_data_without_cat_item['Assigned Dept'].str.strip() == 'EAA_WINDOWS SERVICES_IMPL') | (merged_data_with_cat_item['u_assignment_group'].str.strip() == 'EAA_WINDOWS SERVICES_IMPL'))].index.tolist()\n",
    "    merged_data_without_cat_item.loc[win_tickets_null_client_base , 'Client Base'] = 'Windows'\n",
    "    \n",
    "    print('merged_data_with_cat_item : ', merged_data_with_cat_item.shape)\n",
    "    print('merged_data_without_cat_item : ', merged_data_without_cat_item.shape)\n",
    "    print('Ended mergeTestData')\n",
    "    return merged_data_with_cat_item , merged_data_without_cat_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepareDataByRBCTitle(sm9_data_without_RITM ,merged_data_without_cat_item):\n",
    "    print('Started prepareDataByRBCTitle')\n",
    "    sm9_data_without_RITM = cleanTicketTitle(sm9_data_without_RITM)\n",
    "    merged_data_without_cat_item = cleanTicketTitle(merged_data_without_cat_item)\n",
    "    print('Ended prepareDataByRBCTitle')\n",
    "    return sm9_data_without_RITM , merged_data_without_cat_item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# features : X ; labels : Y ; filename : model name ; location : where model is stored. \n",
    "# Try using joblib to persist the model. \n",
    "def trainModel(features , labels, model , countVec):\n",
    "    print('Started trainModel')\n",
    "    cv = CountVectorizer(ngram_range=(2, 2) , stop_words='english')\n",
    "    X = cv.fit_transform(features)\n",
    "    Y = labels\n",
    "    #len(cv.get_feature_names())\n",
    "    # Removed stratify=Y as the least populated class has only 1 sample. \n",
    "    # X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.03, random_state=42)\n",
    "    #print(X_train.shape , Y_train.shape , X_test.shape , Y_test.shape)\n",
    "    clf = MultinomialNB()\n",
    "    clf = MultinomialNB().fit(X, Y)\n",
    "    # save the model to disk\n",
    "    pickle.dump(clf, open(model, 'wb'))\n",
    "    # save count vectorizer to disk\n",
    "    pickle.dump(cv, open(countVec, 'wb'))\n",
    "    # predict_proba = clf.predict_proba(X_test)\n",
    "    print('Ended trainModel')\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TO-DO : Use joblib to load the model\n",
    "def predictModelByCatItem(features , model , countVec):\n",
    "    # Code to preprocess the data\n",
    "    # Code to load the model \n",
    "    print('Started predictModelByCatItem')\n",
    "    print(\"features : \", features.shape)\n",
    "    cv = pickle.load(open(countVec, 'rb'))\n",
    "    clf = pickle.load(open(model, 'rb'))\n",
    "    \n",
    "    #cv = CountVectorizer(ngram_range=(2, 2) , stop_words='english')\n",
    "    #cv = CountVectorizer(ngram_range=(2, 2) )\n",
    "    X = cv.transform(features)\n",
    "    \n",
    "    predict_proba = clf.predict_proba(X)\n",
    "    analyst_prob = []\n",
    "    for p_p in predict_proba:\n",
    "        analyst_prob.append(zip(clf.classes_ , p_p))\n",
    "    sorted_analyst_prob = []\n",
    "    for lpp in analyst_prob : \n",
    "        sorted_analyst_prob.append(sorted(lpp, key=lambda x: x[1], reverse=True ))\n",
    "    print('Ended predictModelByCatItem')\n",
    "    return sorted_analyst_prob\n",
    "    # Code for model evaluation. Not relevant at this point \n",
    "#     index_match = []\n",
    "#     for i,slp in enumerate(sorted_analyst_prob) :\n",
    "#         index_match.append([lb[0] for lb in slp].index(Y_test.iloc[i]))\n",
    "#     index_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TO-DO : Use joblib to load the model\n",
    "def predictModelByTitle(features , model , countVec):\n",
    "    # Code to preprocess the data\n",
    "    # Code to load the model \n",
    "    print('Started predictModelByTitle')\n",
    "    print(\"features : \", features.shape)\n",
    "    cv = pickle.load(open(countVec, 'rb'))\n",
    "    clf = pickle.load(open(model, 'rb'))\n",
    "    \n",
    "    \n",
    "    #cv = CountVectorizer(ngram_range=(2, 2) , stop_words='english')\n",
    "    X = cv.transform(features)\n",
    "    #clf = pickle.load(open(filename_location, 'rb'))\n",
    "    predict_proba = clf.predict_proba(X)\n",
    "    analyst_prob = []\n",
    "    for p_p in predict_proba:\n",
    "        analyst_prob.append(zip(clf.classes_ , p_p))\n",
    "    sorted_analyst_prob = []\n",
    "    for lpp in analyst_prob : \n",
    "        sorted_analyst_prob.append(sorted(lpp, key=lambda x: x[1], reverse=True ))\n",
    "    print('Started predictModelByTitle')\n",
    "    return sorted_analyst_prob\n",
    "    # Code for model evaluation. Not relevant at this point \n",
    "#     index_match = []\n",
    "#     for i,slp in enumerate(sorted_analyst_prob) :\n",
    "#         index_match.append([lb[0] for lb in slp].index(Y_test.iloc[i]))\n",
    "#     index_match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-process Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are teams we have in ESAM that work on tickets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTeams():\n",
    "    print('Started getTeams')\n",
    "    from datetime import datetime\n",
    "    analyst_teams = pd.read_excel(os.environ['PROJECT_DIR'] +  '\\\\rawdata\\\\Global 2018 Operations Schedule.xlsx', sheetname='MONTHLY SCHEDULE', header = 2 , index_col=0)\n",
    "    team_name = []\n",
    "    team_index = analyst_teams.index.tolist()\n",
    "    for t in team_index:\n",
    "        if type(t) is str:\n",
    "            team_name.append(t)\n",
    "    # Get the month for which we want the team for . \n",
    "    current_month = datetime.now().strftime('%B') # 'August'\n",
    "    team_dict = {} # Dictionary of team name as key & team mates as value. \n",
    "    for t in range(len(team_name) - 1):\n",
    "        team = analyst_teams.loc[team_name[t]:team_name[t+1] , current_month]\n",
    "        team = team.iloc[:len(team) - 1]\n",
    "        team.reset_index(drop=True,inplace=True)\n",
    "        team = team.loc[~team.isnull()]\n",
    "        team = team[:len(team)]\n",
    "        team_dict[team_name[t]] = team\n",
    "    print('Ended getTeams')\n",
    "    return team_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started getTeams\n",
      "Ended getTeams\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'CDN I&TS': 0    MacLellan, Jon \n",
       " 1     Stone, Randall\n",
       " Name: August, dtype: object,\n",
       " 'Cards & Payments, RBC Bank': 0    Mostoles, Sherylle\n",
       " 1    Jette, Christopher\n",
       " Name: August, dtype: object,\n",
       " 'Caribbean': 0            King, Sancha\n",
       " 1    Dubissette, Donnette\n",
       " 2          Dookie, Pamela\n",
       " 3      Adderley, Nikeisha\n",
       " 4        Fonrose, Sadicki\n",
       " Name: August, dtype: object,\n",
       " 'Dominion Securities, E&T, PH&N': 0     Papagiannidis, Kostas\n",
       " 1            Chohan, Sophia\n",
       " 2         Lilbourne, Lauren\n",
       " 3    MariaFrancis, Jennifia\n",
       " Name: August, dtype: object,\n",
       " 'Early': 0    McQuarrie, Katrina\n",
       " Name: August, dtype: object,\n",
       " 'Insurance': 0      Gibbons, Paul\n",
       " 1    Jereza, Agustin\n",
       " Name: August, dtype: object,\n",
       " 'Late': 0    Kadoura, Zaki\n",
       " 1      Shah, Rohan\n",
       " Name: August, dtype: object,\n",
       " 'Mailbox/ Passwords': Series([], Name: August, dtype: object),\n",
       " 'UK I&TS': Series([], Name: August, dtype: object),\n",
       " 'UK Wealth Mgmt': 0     Johnston, Dan\n",
       " 1    Hardisty, Erin\n",
       " 2       Wrona, Anna\n",
       " 3     Dorney, Niamh\n",
       " Name: August, dtype: object,\n",
       " 'US CAS': 0    Dickerson, Kelly\n",
       " 1    Onchwari, Hezron\n",
       " Name: August, dtype: object,\n",
       " 'US Corp': 0    Shupien, Judi\n",
       " Name: August, dtype: object,\n",
       " 'US PCG': 0            Ding, Dan\n",
       " 1    Chea, Christopher\n",
       " Name: August, dtype: object,\n",
       " 'Wealth Management, Direct Investing, Capital Markets, Head Office': 0    Swaby, Oraine\n",
       " 1      Shah, Rohan\n",
       " Name: August, dtype: object,\n",
       " 'Windows': 0     Deschenes, Sylvia\n",
       " 1      Huston, Virgilia\n",
       " 2       Tompkins, Kevin\n",
       " 3     McQuarrie, Andrew\n",
       " 4     Sandhawalia, Soby\n",
       " 5         Aberin, Mario\n",
       " 6          Jamal, Sarah\n",
       " 7           Patel, Rita\n",
       " 9        Leigh, Michael\n",
       " 11        Wright, Diane\n",
       " 12      Bullard, Gerald\n",
       " 13          Byron, Jane\n",
       " 14       Cai, Charlotte\n",
       " 15         Liu, Chen C \n",
       " 16         Jaswal, Aman\n",
       " 17      Chaudry, Arusha\n",
       " Name: August, dtype: object}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getTeams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Client Base to Teams Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getClientBaseToTeamMapping():\n",
    "    print('Started getClientBaseToTeamMapping')\n",
    "    client_base_to_team = pd.read_excel(os.environ['PROJECT_DIR'] +  '\\\\rawdata\\\\Client_Base_to_Team_Mapper.xlsx',index_col=0)\n",
    "    print('Ended getClientBaseToTeamMapping')\n",
    "    return client_base_to_team\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vacation Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get Vacation Schedule: Returns the vacation calender for current month. \n",
    "# Used to filter out analyst on leave. \n",
    "\n",
    "def getVacationSchedule():\n",
    "    print('Started getVacationSchedule')\n",
    "    all_vacation = pd.read_excel(os.environ['PROJECT_DIR'] +  '\\\\rawdata\\\\Vacation Calendar 2018.xlsx', sheetname='All', index_col=0)\n",
    "    all_vacation.columns = list(range(1,32))\n",
    "    # Get the month & year. This would be used to get vacation information for the corresponding month and year. \n",
    "    current_month = datetime.now().strftime('%B') # 'August'\n",
    "    current_year = datetime.now().strftime('%Y')  # 2018\n",
    "    month_year = (current_month + ' ' + str(current_year)).upper()\n",
    "    next_month_year = ''\n",
    "    vc = pd.DataFrame()\n",
    "    # Make sure the year in the below condition is a variable. What if the year is December 2019. Then the below code would fail. \n",
    "    if month_year != 'DECEMBER 2018':\n",
    "        #nextmonth = datetime.date.today() + relativedelta.relativedelta(months=1)\n",
    "        nextmonth = datetime.now() + relativedelta.relativedelta(months=1)\n",
    "        next_month = nextmonth.strftime('%B')\n",
    "        next_month_year = (next_month + ' ' + current_year).upper()\n",
    "        vc = all_vacation.loc[month_year : next_month_year]\n",
    "    else:\n",
    "        # What if the month year is December 2018\n",
    "        # In this case you would use the current month_year variable.\n",
    "        vc = all_vacation.loc[month_year : ]\n",
    "        pass\n",
    "    #print(\"To : \" , next_month_year)    \n",
    "    print('Ended getVacationSchedule')\n",
    "    return vc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started getVacationSchedule\n",
      "Ended getVacationSchedule\n"
     ]
    }
   ],
   "source": [
    "vc = getVacationSchedule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code to check if an analyst is on vacation \n",
    "\n",
    "# Based on current day & the analyst name, check if the analyst is on a vacation. \n",
    "\n",
    "def isAnalystOnLeave(analyst_name):\n",
    "#     print('Started isAnalystOnLeave')\n",
    "    from datetime import datetime\n",
    "    leave_days = [7.5 , 7.50 , 'L' , 'C' , 'T']\n",
    "    current_day = int(datetime.now().strftime('%d'))\n",
    "    \n",
    "    #analyst_name = 'Papagiannidis, Kostas'\n",
    "#     print(\"analyst_name : \", analyst_name)\n",
    "#     print(\"current_day : \", current_day)\n",
    "#     print(\"type(current_day) : \" , type(current_day))\n",
    "#     print(\"current_day.astype(int) : \" , current_day.astype(int))\n",
    "    vc\n",
    "    try:\n",
    "        if vc.loc[analyst_name][current_day] in leave_days : \n",
    "            #print(\"Yes\")\n",
    "#             print('Ended isAnalystOnLeave : True')\n",
    "            print(analyst_name + ' is on leave. No tickets would be assigned.')\n",
    "            return True\n",
    "        else:\n",
    "#             print('Ended isAnalystOnLeave : False')\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        #print(\"Exceptin in isAnalystOnLeave : \" , e)\n",
    "        return False\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get all tickets in an analyst's queue. \n",
    "# Some files use csv , where as others use .xlsx . Lets standardize to excel sheets as they occupy less space. Check & ensure \n",
    "# the same applies on the linux box too\n",
    "def getAvailabilityPerAnalyst():\n",
    "    print('Started getAvailabilityPerAnalyst')\n",
    "    availability_columns = ['Assigned to','Pending Customer','Client Base']\n",
    "    #availability = pd.read_csv(os.environ['PROJECT_DIR'] +  '\\\\rawdata\\\\availability.csv' , encoding='latin-1' , usecols=availability_columns )\n",
    "    availability = pd.read_excel(os.environ['PROJECT_DIR'] +  '\\\\rawdata\\\\availability.xlsx' , usecols=availability_columns )\n",
    "    availability = availability.loc[~availability['Assigned to'].isnull()]\n",
    "    availability['Pending Customer'] = availability['Pending Customer'].fillna('FALSE')\n",
    "    availability['Assigned to'] = availability['Assigned to'].apply(object_to_int)\n",
    "    #availability.loc[availability['Assigned to'].isin(['310294822'])]\n",
    "    availability_grouped = availability.groupby(['Assigned to','Pending Customer']).count()\n",
    "    availability_grouped.rename(columns = {'Client Base' : 'No_of_Tickets_Assigned'} , inplace=True)\n",
    "    #availability_grouped\n",
    "    # Below line is commented as we made team_sheet_without_leads global\n",
    "    #team_sheet_without_leads = getAnalyst()\n",
    "    analyst_emp_id = team_sheet_without_leads.index.tolist()\n",
    "    tickets_analyst_queue = {}\n",
    "    for analyst in analyst_emp_id:\n",
    "        try:\n",
    "            tickets_analyst_queue[analyst] = availability_grouped.loc[(analyst,'FALSE')]['No_of_Tickets_Assigned']\n",
    "        except Exception as e:\n",
    "            print(\"Exception in getAvailabilityPerAnalyst : \" , e)\n",
    "            tickets_analyst_queue[analyst] = 0\n",
    "    print('Availability')\n",
    "    print(tickets_analyst_queue)\n",
    "    print('Ended getAvailabilityPerAnalyst')\n",
    "    return tickets_analyst_queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def postProcess(sorted_analyst_prob, ticket):\n",
    "    # This variable will save a list of recommendations for each ticket\n",
    "    print('Started postProcess')\n",
    "#     print(\"ticket.head() : \")\n",
    "#     print(ticket)\n",
    "    #ticket.head()\n",
    "    r1 , r2 , r3 , r4 = [] , [] , [] ,[]\n",
    "    recommendations_list = []\n",
    "    client_base_to_team = getClientBaseToTeamMapping()\n",
    "    team_dict = getTeams()\n",
    "#     print(\"team_dict\")\n",
    "#     print(team_dict)\n",
    "#     print(\"team_sheet_without_leads\")\n",
    "#     print(team_sheet_without_leads)\n",
    "    #tickets_to_recommend = ticket.index.tolist()\n",
    "    tickets_analyst_queue = getAvailabilityPerAnalyst()\n",
    "    count = 0\n",
    "#     for count , t in enumerate(ticket):\n",
    "    for index, t in ticket.iterrows():\n",
    "#         print(\"t  \" , t)\n",
    "#         print(\"t['Client Base'] :  \", t['Client Base'])\n",
    "        c_b = t['Client Base']\n",
    "#         print(\"c_b : \", c_b)\n",
    "#         print(\"type(c_b) : \", type(c_b))\n",
    "        # TO-DO What if the client base is null\n",
    "        #if c_b != np.nan:\n",
    "        if pd.isnull(c_b):\n",
    "            t_l = ['All']\n",
    "        else:\n",
    "            team = client_base_to_team.loc[c_b,'ESAM Application Teams ']\n",
    "            t_l = team.split('|')\n",
    "        # Get me all the analyst to be considered for this ticket\n",
    "        a_l = []\n",
    "        if t_l[0] in 'All':\n",
    "            a_l = team_sheet_without_leads['Name'].tolist()\n",
    "            # Consider all analyst across all teams\n",
    "            # iterate through all keys of team_list dictionary. Set a_l to the list of analyst across teams. \n",
    "        else:\n",
    "            for team in t_l:\n",
    "                team = team.strip()\n",
    "                for t_m in team_dict[team]:\n",
    "                    a_l.append(t_m)\n",
    "            # We now have all analyst we need to consider. \n",
    "#             print(a_l)\n",
    "        # Check Vacation Calender & remove those on Vacation\n",
    "        # Use List comprehension insead of the below line\n",
    "        analyst_not_on_vacation = []\n",
    "        for a in a_l:\n",
    "            if isAnalystOnLeave(a):\n",
    "                continue\n",
    "            else:\n",
    "                analyst_not_on_vacation.append(a)\n",
    "        #analyst_not_on_vacation\n",
    "        # Getting employee id based on analyst name. \n",
    "        analyst_not_on_leave = []\n",
    "        # Below line is commented as we made team_sheet_without_leads global\n",
    "        #team_sheet_without_leads = getAnalyst()\n",
    "        for a in analyst_not_on_vacation:\n",
    "            try:\n",
    "                emp_id =(team_sheet_without_leads.loc[team_sheet_without_leads['Name'].str.strip() == a.strip()]).index.tolist()[0]\n",
    "                analyst_not_on_leave.append(emp_id)\n",
    "            except Exception as e:\n",
    "                print(\"Exception in postprocess while looping over analyst_not_on_vacation\" , e)\n",
    "                print(\"a :\",a)\n",
    "#                 print(\"team_dict : \", team_dict)\n",
    "#                 print(\"team_sheet_without_leads : \",team_sheet_without_leads)\n",
    "            #print(emp_id)\n",
    "            \n",
    "        #analyst_not_on_leave\n",
    "        # Get probability of analyst to be considered , sort them in descending order. It already sorted. Evaluate your approach\n",
    "        # Do you need to iterate through it OR you can pick the analyst. Some thought needed. \n",
    "        analyst_prob = []\n",
    "        for i in range(len(sorted_analyst_prob[count])):\n",
    "            if sorted_analyst_prob[count][i][0] in analyst_not_on_leave:\n",
    "                analyst_prob.append(sorted_analyst_prob[count][i])\n",
    "        #analyst_prob  \n",
    "        # Check availability & don't consider analyst having more than 45 tickets. Need to amend this later to give lower probability to \n",
    "        # those having more tickets in their bin. Once you have 4 analyst who can resolve such tickets & are available.\n",
    "        analyst_avail = {}\n",
    "        for a in analyst_prob:\n",
    "            try:\n",
    "                analyst_avail[a[0]] = tickets_analyst_queue[a[0]]\n",
    "            except Exception as e:\n",
    "                print(\"Exception in postprocess while looping over analyst_prob \", e)\n",
    "                analyst_avail[a[0]] = -1\n",
    "            #tickets_analyst_queue\n",
    "        #analyst_avail\n",
    "        recommendations = []\n",
    "        for a_p in analyst_prob:\n",
    "            if analyst_avail[a_p[0]] <= 45:\n",
    "                recommendations.append(team_sheet_without_leads.loc[a_p[0]]['Name'])\n",
    "        recommendations_list.append(recommendations)\n",
    "        count = count + 1\n",
    "\n",
    "    # Add recommendations to the tickets \n",
    "    print('Size of Recommendations_list : ',len(recommendations_list))\n",
    "    print('Number of tickets in shape : ', ticket.shape)\n",
    "    for r in recommendations_list:\n",
    "        try:\n",
    "            r1.append(r[0])\n",
    "        except Exception as e:\n",
    "            r1.append('None 1')\n",
    "            print(\"r1.append(r[0]) : \", e)\n",
    "        try:\n",
    "            r2.append(r[1])\n",
    "        except Exception as e:\n",
    "            r2.append('None 2')\n",
    "            print(\"r2.append(r[1]) : \", e)\n",
    "        try:\n",
    "            r3.append(r[2])\n",
    "        except Exception as e:\n",
    "            r3.append('None 3')\n",
    "            print(\"r3.append(r[2]) : \", e)\n",
    "        try:\n",
    "            r4.append(r[3])\n",
    "        except Exception as e:\n",
    "            r4.append('None 4')\n",
    "            print(\"r3.append(r[3]) : \", e)   \n",
    "        \n",
    "            \n",
    "    \n",
    "    R1 = pd.Series(r1 , index=ticket.index)    \n",
    "    R2 = pd.Series(r2 , index=ticket.index)    \n",
    "    R3 = pd.Series(r3 , index=ticket.index)    \n",
    "    R4 = pd.Series(r4 , index=ticket.index)\n",
    "#     print('R4 : ', R4)\n",
    "\n",
    "    ticket['R1'] = R1\n",
    "    ticket['R2'] = R2\n",
    "    ticket['R3'] = R3\n",
    "    ticket['R4'] = R4 \n",
    "    print('Ended postProcess')\n",
    "    return ticket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Useful code that would comes in use when we have the team data frame. Used to get name to the employee based on id & vice-versa\n",
    "\n",
    "# emp_id = (team_sheet_without_leads.loc[team_sheet_without_leads['Name'] == 'Aberin, Mario']).index.tolist()\n",
    "# name = team_sheet_without_leads.loc[191918473]['Name'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Pre-process Data \n",
    "    print('Started train')\n",
    "    sm9_data_with_RITM , sm9_data_without_RITM = getSM9TrainData(os.environ['PROJECT_DIR'] + os.environ['SM9_TRAINING_DATA_EXCEL'])\n",
    "    sn_data = getServiceNowTrainData(os.environ['PROJECT_DIR'] + os.environ['SERVICE_NOW_TRAINING_DATA_EXCEL'])\n",
    "    merged_data_with_cat_item , merged_data_without_cat_item = mergeTrainingData(sm9_data_with_RITM,sn_data)\n",
    "    sm9_data_without_RITM , merged_data_without_cat_item = prepareDataByRBCTitle(sm9_data_without_RITM,merged_data_without_cat_item)\n",
    "#     sm9_data_without_RITM = sm9_data_without_RITM[['RBC Line Item Title','Assigned to']]\n",
    "#     merged_data_without_cat_item = merged_data_without_cat_item[['RBC Line Item Title','Assigned to']]\n",
    "    tickets_by_title = pd.concat([sm9_data_without_RITM , merged_data_without_cat_item])\n",
    "    clf_by_cat_item = trainModel(merged_data_with_cat_item['cat_item'],merged_data_with_cat_item['Assigned to'] , os.environ['PROJECT_DIR'] + \"models\\\\by_cat_item.pkl\",os.environ['PROJECT_DIR'] + \"models\\\\count_vec_cat_item.pkl\")\n",
    "    clf_by_title = trainModel(tickets_by_title['RBC Line Item Title'],tickets_by_title['Assigned to'],os.environ['PROJECT_DIR'] + \"models\\\\by_title.pkl\", os.environ['PROJECT_DIR'] + \"models\\\\count_vec_by_title.pkl\")\n",
    "    print('Ended train')\n",
    "#     return tickets_by_title\n",
    "    # train models & save them \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "def predict():\n",
    "    # Pre-process Data \n",
    "    print('Started predict')\n",
    "    sm9_data_with_RITM , sm9_data_without_RITM = getSM9TestData(os.environ['PROJECT_DIR'] + os.environ['SM9_TEST_DATA_EXCEL'])\n",
    "    sn_data = getServiceNowTestData(os.environ['PROJECT_DIR'] + os.environ['SERVICE_NOW_TEST_DATA_EXCEL'])\n",
    "    merged_data_with_cat_item , merged_data_without_cat_item = mergeTestData(sm9_data_with_RITM,sn_data)\n",
    "    sm9_data_without_RITM , merged_data_without_cat_item = prepareDataByRBCTitle(sm9_data_without_RITM,merged_data_without_cat_item)\n",
    "#     sm9_data_without_RITM = sm9_data_without_RITM[['RBC Line Item Title']]\n",
    "#     merged_data_without_cat_item = merged_data_without_cat_item[['RBC Line Item Title']]\n",
    "    tickets_by_title = pd.concat([sm9_data_without_RITM , merged_data_without_cat_item])\n",
    "    sorted_analyst_prob_by_item = predictModelByCatItem(merged_data_with_cat_item['cat_item'],os.environ['PROJECT_DIR'] + \"models\\\\by_cat_item.pkl\",os.environ['PROJECT_DIR'] + \"models\\\\count_vec_cat_item.pkl\")\n",
    "    cat_item_ticket_recommendations = postProcess(sorted_analyst_prob_by_item,merged_data_with_cat_item)\n",
    "    sorted_analyst_prob_by_title = predictModelByTitle(tickets_by_title['RBC Line Item Title'],os.environ['PROJECT_DIR'] + \"models\\\\by_title.pkl\",os.environ['PROJECT_DIR'] + \"models\\\\count_vec_by_title.pkl\")\n",
    "    rbc_title_ticket_recommendations = postProcess(sorted_analyst_prob_by_title,tickets_by_title)\n",
    "    ticket_recommendations = pd.concat([cat_item_ticket_recommendations,rbc_title_ticket_recommendations])\n",
    "    filename = os.environ['PROJECT_DIR'] + os.environ['RECOMMENDATIONS'] + \"tickets_recommendations_\" + datetime.datetime.today().strftime('%Y-%m-%d') + \".xlsx\"\n",
    "#     filename = \"tickets_recommendation_\" + datetime.datetime.now().isoformat() + \".xlsx\"\n",
    "    # Prefarably save as excel \n",
    "    #ticket_recommendations.to_csv(path_or_buf=filename  , encoding=\"Latin-1\" , index = False)\n",
    "    #ticket_recommendations.to_excel(path_or_buf=filename , index = False)\n",
    "    # Save as excel \n",
    "    writer = pd.ExcelWriter(filename, engine='xlsxwriter')\n",
    "    ticket_recommendations.to_excel(writer,sheet_name='Sheet1',index = False)\n",
    "    writer.save()\n",
    "    print('Ended predict')\n",
    "    # Done saving recommendations as excel. \n",
    "    # Send email to email list. \n",
    "    \n",
    "#     os.environ['SERVICE_NOW_TRAINING_DATA'] = \"rawdata\\\\serviceNow\\\\train\\\\*.csv\"\n",
    "#     os.environ['SERVICE_NOW_TEST_DATA'] = \"rawdata\\\\serviceNow\\\\test\\\\*.csv\"\n",
    "    # Load Models & pass data\n",
    "    # Post-process recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train()\n",
    "#predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# my_list = [['Aazim', 'Shiraz'],['Mom','Dad'],['Bhabhi','Divine']]\n",
    "# names = []\n",
    "# for l in range(len(my_list)):\n",
    "#     print(my_list[l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import datetime\n",
    "# #now=datetime.datetime.now()\n",
    "# #now.isoformat()\n",
    "# print('fsfsds ' + datetime.datetime.now().isoformat() + ' ffewfwe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import datetime\n",
    "\n",
    "# # # Create a Pandas dataframe from some data.\n",
    "# df = pd.DataFrame({'Data': [10, 20, 30, 20, 15, 30, 45]})\n",
    "\n",
    "# # # Create a Pandas Excel writer using XlsxWriter as the engine.\n",
    "# # writer = pd.ExcelWriter('pandas_simple.xlsx', engine='xlsxwriter')\n",
    "# filename = os.environ['PROJECT_DIR'] + os.environ['RECOMMENDATIONS'] + \"tickets_recommendations_\" + datetime.datetime.today().strftime('%Y-%m-%d') + \".xlsx\"\n",
    "# print(filename)\n",
    "# writer = pd.ExcelWriter(filename , engine='xlsxwriter')\n",
    "# df.to_excel(writer,sheet_name='Sheet1' )\n",
    "# writer.save()\n",
    "\n",
    "# # Convert the dataframe to an XlsxWriter Excel object.\n",
    "# df.to_excel(writer, sheet_name='Sheet1')\n",
    "\n",
    "# # Close the Pandas Excel writer and output the Excel file.\n",
    "# writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import datetime\n",
    "# d = datetime.datetime.now()\n",
    "# datetime.datetime.today().strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# from flask import Flask, request, render_template, send_from_directory\n",
    "\n",
    "# #__author__ = 'ibininja'\n",
    "\n",
    "# app = Flask(__name__)\n",
    "\n",
    "# APP_ROOT = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "# print(APP_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\581686284\\\\PycharmProjects\\\\Automated_Ticket_Management-2\\\\jupyter'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "#os.path.abspath()\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# os.path.abspath('') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# APP_ROOT = os.path.dirname(os.path.abspath(''))\n",
    "# APP_ROOT\n",
    "# filename = 'otpa_fs_32313'\n",
    "# filename_to_target = {\n",
    "#                             'sm9_learn' : 'data/sm9/train',\n",
    "#                             'sm9_predict' : 'data/sm9/test',\n",
    "#                             'sn_learn' : 'data/serviceNow/train',\n",
    "#                             'sn_predict' : 'data/serviceNow/test',\n",
    "#                             'otpa' : 'data/postprocess/otpa',\n",
    "#                             'vacation calendar' : 'data/postprocess',\n",
    "#                             'global' : 'data/postprocess',\n",
    "#                             'team list' : 'data/postprocess',\n",
    "#                             'client_base_to_team_mapper' : 'data/postprocess'\n",
    "#                          }\n",
    "# file_type = [key for key in filename_to_target.keys() if filename.lower().startswith(key)]\n",
    "# # print(file_type.pop() )\n",
    "# # print(file_type)\n",
    "# print('filename_to_target : ',  filename_to_target[file_type.pop()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alabaster==0.7.10\n",
      "anaconda-client==1.6.5\n",
      "anaconda-navigator==1.6.8\n",
      "anaconda-project==0.8.0\n",
      "aniso8601==3.0.2\n",
      "asn1crypto==0.22.0\n",
      "astroid==1.5.3\n",
      "astropy==2.0.2\n",
      "babel==2.5.0\n",
      "backports.shutil-get-terminal-size==1.0.0\n",
      "beautifulsoup4==4.6.0\n",
      "bitarray==0.8.1\n",
      "bkcharts==0.2\n",
      "blaze==0.11.3\n",
      "bleach==2.0.0\n",
      "bokeh==0.12.7\n",
      "boto==2.48.0\n",
      "boto3==1.7.29\n",
      "botocore==1.10.29\n",
      "Bottleneck==1.2.1\n",
      "bz2file==0.98\n",
      "CacheControl==0.12.3\n",
      "certifi==2017.7.27.1\n",
      "cffi==1.10.0\n",
      "chardet==3.0.4\n",
      "click==6.7\n",
      "cloudpickle==0.4.0\n",
      "clyent==1.2.2\n",
      "colorama==0.3.9\n",
      "comtypes==1.1.2\n",
      "conda==4.3.27\n",
      "conda-build==3.0.22\n",
      "conda-verify==2.0.0\n",
      "contextlib2==0.5.5\n",
      "cryptography==2.0.3\n",
      "cycler==0.10.0\n",
      "Cython==0.26.1\n",
      "cytoolz==0.8.2\n",
      "dask==0.15.2\n",
      "datashape==0.5.4\n",
      "decorator==4.1.2\n",
      "distlib==0.2.5\n",
      "distributed==1.18.3\n",
      "docutils==0.14\n",
      "entrypoints==0.2.3\n",
      "et-xmlfile==1.0.1\n",
      "fastcache==1.0.2\n",
      "filelock==2.0.12\n",
      "Flask==0.12.2\n",
      "Flask-Cors==3.0.3\n",
      "Flask-Jsonpify==1.5.0\n",
      "flask-marshmallow==0.9.0\n",
      "Flask-RESTful==0.3.6\n",
      "gensim==3.4.0\n",
      "gevent==1.2.2\n",
      "glob2==0.5\n",
      "greenlet==0.4.12\n",
      "h5py==2.7.0\n",
      "heapdict==1.0.0\n",
      "html5lib==0.999999999\n",
      "idna==2.6\n",
      "imageio==2.2.0\n",
      "imagesize==0.7.1\n",
      "ipykernel==4.6.1\n",
      "ipython==6.1.0\n",
      "ipython-genutils==0.2.0\n",
      "ipywidgets==7.0.0\n",
      "isort==4.2.15\n",
      "itsdangerous==0.24\n",
      "jdcal==1.3\n",
      "jedi==0.10.2\n",
      "Jinja2==2.9.6\n",
      "jmespath==0.9.3\n",
      "jsonschema==2.6.0\n",
      "jupyter-client==5.1.0\n",
      "jupyter-console==5.2.0\n",
      "jupyter-core==4.3.0\n",
      "jupyterlab==0.27.0\n",
      "jupyterlab-launcher==0.4.0\n",
      "lazy-object-proxy==1.3.1\n",
      "llvmlite==0.20.0\n",
      "locket==0.2.0\n",
      "lockfile==0.12.2\n",
      "lxml==3.8.0\n",
      "MarkupSafe==1.0\n",
      "marshmallow==2.15.3\n",
      "matplotlib==2.0.2\n",
      "mccabe==0.6.1\n",
      "menuinst==1.4.8\n",
      "mistune==0.7.4\n",
      "mpmath==0.19\n",
      "msgpack-python==0.4.8\n",
      "multipledispatch==0.4.9\n",
      "navigator-updater==0.1.0\n",
      "nbconvert==5.3.1\n",
      "nbformat==4.4.0\n",
      "networkx==1.11\n",
      "nltk==3.2.4\n",
      "nose==1.3.7\n",
      "notebook==5.0.0\n",
      "numba==0.35.0+10.g143f70e\n",
      "numexpr==2.6.2\n",
      "numpy==1.13.1\n",
      "numpydoc==0.7.0\n",
      "odo==0.5.1\n",
      "olefile==0.44\n",
      "openpyxl==2.4.8\n",
      "packaging==16.8\n",
      "pandas==0.20.3\n",
      "pandocfilters==1.4.2\n",
      "partd==0.3.8\n",
      "path.py==10.3.1\n",
      "pathlib2==2.3.0\n",
      "patsy==0.4.1\n",
      "pep8==1.7.0\n",
      "pickleshare==0.7.4\n",
      "Pillow==4.2.1\n",
      "pkginfo==1.4.1\n",
      "plotly==3.0.0\n",
      "ply==3.10\n",
      "progress==1.3\n",
      "prompt-toolkit==1.0.15\n",
      "psutil==5.2.2\n",
      "py==1.4.34\n",
      "pycodestyle==2.3.1\n",
      "pycosat==0.6.2\n",
      "pycparser==2.18\n",
      "pycrypto==2.6.1\n",
      "pycurl==7.43.0\n",
      "pyflakes==1.5.0\n",
      "Pygments==2.2.0\n",
      "pylint==1.7.2\n",
      "pyodbc==4.0.17\n",
      "pyOpenSSL==17.2.0\n",
      "pyparsing==2.2.0\n",
      "PySocks==1.6.7\n",
      "pytest==3.2.1\n",
      "python-dateutil==2.6.1\n",
      "pytz==2017.2\n",
      "PyWavelets==0.5.2\n",
      "pywin32==221\n",
      "PyYAML==3.12\n",
      "pyzmq==16.0.2\n",
      "QtAwesome==0.4.4\n",
      "qtconsole==4.3.1\n",
      "QtPy==1.3.1\n",
      "requests==2.18.4\n",
      "retrying==1.3.3\n",
      "rope==0.10.5\n",
      "ruamel-yaml==0.11.14\n",
      "s3transfer==0.1.13\n",
      "scikit-image==0.13.0\n",
      "scikit-learn==0.19.0\n",
      "scipy==0.19.1\n",
      "seaborn==0.8\n",
      "simplegeneric==0.8.1\n",
      "singledispatch==3.4.0.3\n",
      "six==1.10.0\n",
      "smart-open==1.5.7\n",
      "snowballstemmer==1.2.1\n",
      "sortedcollections==0.5.3\n",
      "sortedcontainers==1.5.7\n",
      "Sphinx==1.6.3\n",
      "sphinxcontrib-websupport==1.0.1\n",
      "spyder==3.2.3\n",
      "SQLAlchemy==1.1.13\n",
      "statsmodels==0.8.0\n",
      "sympy==1.1.1\n",
      "tables==3.4.2\n",
      "tblib==1.3.2\n",
      "testpath==0.3.1\n",
      "toolz==0.8.2\n",
      "tornado==4.5.2\n",
      "traitlets==4.3.2\n",
      "typing==3.6.2\n",
      "unicodecsv==0.14.1\n",
      "urllib3==1.22\n",
      "wcwidth==0.1.7\n",
      "webencodings==0.5.1\n",
      "Werkzeug==0.12.2\n",
      "widgetsnbextension==3.0.2\n",
      "win-inet-pton==1.0.1\n",
      "win-unicode-console==0.5\n",
      "wincertstore==0.2\n",
      "wrapt==1.10.11\n",
      "xlrd==1.1.0\n",
      "XlsxWriter==0.9.8\n",
      "xlwings==0.11.4\n",
      "xlwt==1.3.0\n",
      "zict==0.1.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 9.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zict\n",
      "xlwt\n",
      "xlwings\n",
      "XlsxWriter\n",
      "xlrd\n",
      "wrapt\n",
      "wincertstore\n",
      "win-unicode-console\n",
      "win-inet-pton\n",
      "widgetsnbextension\n",
      "wheel\n",
      "Werkzeug\n",
      "webencodings\n",
      "wcwidth\n",
      "urllib3\n",
      "unicodecsv\n",
      "typing\n",
      "traitlets\n",
      "tornado\n",
      "toolz\n",
      "testpath\n",
      "tblib\n",
      "tables\n",
      "sympy\n",
      "statsmodels\n",
      "SQLAlchemy\n",
      "spyder\n",
      "sphinxcontrib-websupport\n",
      "Sphinx\n",
      "sortedcontainers\n",
      "sortedcollections\n",
      "snowballstemmer\n",
      "smart-open\n",
      "six\n",
      "singledispatch\n",
      "simplegeneric\n",
      "setuptools\n",
      "seaborn\n",
      "scipy\n",
      "scikit-learn\n",
      "scikit-image\n",
      "s3transfer\n",
      "ruamel-yaml\n",
      "rope\n",
      "retrying\n",
      "requests\n",
      "QtPy\n",
      "qtconsole\n",
      "QtAwesome\n",
      "pyzmq\n",
      "PyYAML\n",
      "pywin32\n",
      "PyWavelets\n",
      "pytz\n",
      "python-dateutil\n",
      "pytest\n",
      "PySocks\n",
      "pyparsing\n",
      "pyOpenSSL\n",
      "pyodbc\n",
      "pylint\n",
      "Pygments\n",
      "pyflakes\n",
      "pycurl\n",
      "pycrypto\n",
      "pycparser\n",
      "pycosat\n",
      "pycodestyle\n",
      "py\n",
      "psutil\n",
      "prompt-toolkit\n",
      "progress\n",
      "ply\n",
      "plotly\n",
      "pkginfo\n",
      "pip\n",
      "Pillow\n",
      "pickleshare\n",
      "pep8\n",
      "patsy\n",
      "pathlib2\n",
      "path.py\n",
      "partd\n",
      "pandocfilters\n",
      "pandas\n",
      "packaging\n",
      "openpyxl\n",
      "olefile\n",
      "odo\n",
      "numpydoc\n",
      "numpy\n",
      "numexpr\n",
      "numba\n",
      "notebook\n",
      "nose\n",
      "nltk\n",
      "networkx\n",
      "nbformat\n",
      "nbconvert\n",
      "navigator-updater\n",
      "multipledispatch\n",
      "msgpack-python\n",
      "mpmath\n",
      "mistune\n",
      "menuinst\n",
      "mccabe\n",
      "matplotlib\n",
      "marshmallow\n",
      "MarkupSafe\n",
      "lxml\n",
      "lockfile\n",
      "locket\n",
      "llvmlite\n",
      "lazy-object-proxy\n",
      "jupyterlab\n",
      "jupyterlab-launcher\n",
      "jupyter-core\n",
      "jupyter-console\n",
      "jupyter-client\n",
      "jsonschema\n",
      "jmespath\n",
      "Jinja2\n",
      "jedi\n",
      "jdcal\n",
      "itsdangerous\n",
      "isort\n",
      "ipywidgets\n",
      "ipython\n",
      "ipython-genutils\n",
      "ipykernel\n",
      "imagesize\n",
      "imageio\n",
      "idna\n",
      "html5lib\n",
      "heapdict\n",
      "h5py\n",
      "greenlet\n",
      "glob2\n",
      "gevent\n",
      "gensim\n",
      "Flask\n",
      "Flask-RESTful\n",
      "flask-marshmallow\n",
      "Flask-Jsonpify\n",
      "Flask-Cors\n",
      "filelock\n",
      "fastcache\n",
      "et-xmlfile\n",
      "entrypoints\n",
      "docutils\n",
      "distributed\n",
      "distlib\n",
      "decorator\n",
      "datashape\n",
      "dask\n",
      "cytoolz\n",
      "Cython\n",
      "cycler\n",
      "cryptography\n",
      "contextlib2\n",
      "conda\n",
      "conda-verify\n",
      "conda-build\n",
      "comtypes\n",
      "colorama\n",
      "clyent\n",
      "cloudpickle\n",
      "click\n",
      "chardet\n",
      "cffi\n",
      "certifi\n",
      "CacheControl\n",
      "bz2file\n",
      "Bottleneck\n",
      "botocore\n",
      "boto3\n",
      "boto\n",
      "bokeh\n",
      "bleach\n",
      "blaze\n",
      "bkcharts\n",
      "bitarray\n",
      "beautifulsoup4\n",
      "backports.shutil-get-terminal-size\n",
      "astropy\n",
      "astroid\n",
      "asn1crypto\n",
      "aniso8601\n",
      "anaconda-project\n",
      "anaconda-navigator\n",
      "anaconda-client\n",
      "alabaster\n",
      "babel\n"
     ]
    }
   ],
   "source": [
    "import pip\n",
    "for package in pip.get_installed_distributions():\n",
    "    name = package.project_name # SQLAlchemy, Django, Flask-OAuthlib\n",
    "    print(name)\n",
    "    key = package.key # sqlalchemy, django, flask-oauthlib\n",
    "    module_name = package._get_metadata(\"top_level.txt\") # sqlalchemy, django, flask_oauthlib\n",
    "    location = package.location # virtualenv lib directory etc.\n",
    "    version = package.version # ve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-39b6a31c2ddb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdirname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'relative/path/to/file/you/want'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "dirname = os.path.dirname(__file__)\n",
    "filename = os.path.join(dirname, 'relative/path/to/file/you/want')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
